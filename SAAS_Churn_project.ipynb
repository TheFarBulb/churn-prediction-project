{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "075ac08c-1898-4fb2-b276-29ad5e2e6db5",
   "metadata": {},
   "source": [
    "# 💼 Customer Churn Analysis for Telecom\n",
    "**Objective**: Predict and explain churn using EDA, ML, and SHAP  \n",
    "**Tech Stack**: Python, LightGBM, SHAP, scikit-learn, pandas, seaborn  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe162fd-840c-4cd4-8ec8-4f478a119dae",
   "metadata": {},
   "source": [
    "## Step 1: Imports and Setup\n",
    "This section includes all necessary libraries and global configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df964cd-e5d2-4ee9-a1b8-eca92c59edc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RICH/opt/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "/Users/RICH/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve,ConfusionMatrixDisplay, precision_recall_curve, f1_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from itertools import combinations\n",
    "\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "# --- Warnings Configuration ---\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='statsmodels')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='lightgbm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e05a9-0832-47ac-ae49-b059c895380b",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset\n",
    "We'll use a Telco churn dataset containing customer demographics, service details, and churn labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af1305ac-6046-47ed-82da-a3af19d8e938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/RICH/Projects/Freelance_DS/Portofolio/churn_analysis_saas_t1/data/WA_Fn-UseC_-Telco-Customer-Churn.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aa4bb9f90ecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/Users/RICH/Projects/Freelance_DS/Portofolio/churn_analysis_saas_t1/data/WA_Fn-UseC_-Telco-Customer-Churn.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/RICH/Projects/Freelance_DS/Portofolio/churn_analysis_saas_t1/data/WA_Fn-UseC_-Telco-Customer-Churn.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'/Users/RICH/Projects/Freelance_DS/Portofolio/churn_analysis_saas_t1/data/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5526d7b0-bcc9-464b-9e37-76a1f5c8ba19",
   "metadata": {},
   "source": [
    "## Step 3: Clean and Prepare Data\n",
    "We convert numeric fields, impute missing values, encode categorical variables, and drop the non-informative customer ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f2820-966e-405b-82cc-10dc796ff974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Step 3: Clean and Prepare Data ---\n",
    "\n",
    "# 1. Fix data types\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
    "\n",
    "# 2. Drop identifier column\n",
    "df.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "# 3. Map binary categorical columns safely (Yes/No → 1/0)\n",
    "binary_cols = [\n",
    "    'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling',\n",
    "    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "    'TechSupport', 'StreamingTV', 'StreamingMovies', 'Churn'\n",
    "]\n",
    "\n",
    "for col in binary_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# 4. Double-check for remaining missing values\n",
    "print(\"✅ Missing values:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4ec975-cd7e-48d7-aa0c-2ee25135bcef",
   "metadata": {},
   "source": [
    "## Step 4: Overall Churn Rate\n",
    "We start by examining the proportion of customers who churned in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303371c-3f9f-4b4c-9ef8-f31281752101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Overall Churn Rate ---\n",
    "churn_rate = df['Churn'].mean()\n",
    "print(f\"Overall Churn Rate: {churn_rate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7978c51d-ee65-4422-8fae-d13ab6b5b599",
   "metadata": {},
   "source": [
    "## Step 5: Exploratory Churn Analysis\n",
    "\n",
    "We analyze how customer churn relates to both categorical and continuous features.\n",
    "\n",
    "- **Categorical Variables**: Contract type, Internet service, Payment method, Senior citizen status, Online security, and Tech support.\n",
    "- **Continuous Variable**: Tenure.\n",
    "\n",
    "These insights help highlight which customer segments are at higher risk of churning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ff6f4-ad8e-45b8-a677-ab48005dd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Exploratory Churn Analysis by Category and Numeric Features ---\n",
    "\n",
    "# Categorical features to plot\n",
    "categorical_cols = [\n",
    "    'Contract', 'InternetService', 'PaymentMethod', \n",
    "    'OnlineSecurity', 'TechSupport', 'SeniorCitizen'\n",
    "]\n",
    "\n",
    "# Reusable plot function for categorical churn breakdown\n",
    "def plot_churn_by_category(col):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(x=col, hue='Churn', data=df)\n",
    "    plt.title(f'Churn by {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Number of Customers')\n",
    "    plt.legend(title='Churn', labels=['No', 'Yes'])\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate plots for all selected categorical columns\n",
    "for col in categorical_cols:\n",
    "    plot_churn_by_category(col)\n",
    "\n",
    "# Plot for tenure (continuous variable)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=df, x='tenure', hue='Churn', multiple='stack', bins=30)\n",
    "plt.title('Churn by Tenure')\n",
    "plt.xlabel('Tenure (Months)')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e205d3-a4c8-4518-9631-86b4ddbeb063",
   "metadata": {},
   "source": [
    "## Step 6: Correlation Analysis\n",
    "\n",
    "We compute the correlation matrix for all numeric features. This helps:\n",
    "- Detect multicollinearity\n",
    "- Understand how strongly churn correlates with other variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be6a92-91f6-4c46-8724-11168b53b35a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Step 6: Correlation Analysis for Numeric Features ---\n",
    "\n",
    "# Select only numeric features\n",
    "numeric_features = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(numeric_features.corr(), annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
    "plt.title('Correlation Heatmap of Numeric Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad78f02-0ee1-411c-a35a-29dd02cf64b8",
   "metadata": {},
   "source": [
    "## Step 7: Statistical Significance Testing\n",
    "\n",
    "We perform:\n",
    "\n",
    "- **Chi-Square tests** to evaluate whether churn is associated with each categorical feature.\n",
    "- **Pairwise Z-tests** to assess whether specific levels (e.g., “Fiber optic” vs. “DSL”) differ significantly in churn.\n",
    "\n",
    "Results are visualized and saved to CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f7db4-2618-42c9-84c9-6fc1e23b5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: Chi-Square & Z-Test for Categorical Features ---\n",
    "\n",
    "from itertools import combinations\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "alpha = 0.05  # significance level\n",
    "results = []\n",
    "\n",
    "# Ensure churn is numeric\n",
    "if df['Churn'].dtype == object or df['Churn'].isna().all():\n",
    "    df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# List of categorical features to test\n",
    "categorical_features = [\n",
    "    'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',\n",
    "    'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "    'PaperlessBilling', 'PaymentMethod', 'Contract'\n",
    "]\n",
    "\n",
    "for col in categorical_features:\n",
    "    print(f\"\\n===== Feature: {col} =====\")\n",
    "\n",
    "    # Encode values to ensure consistency\n",
    "    encoded_col = f\"{col}_encoded\"\n",
    "    df[encoded_col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
    "    mapping = dict(zip(df[encoded_col], df[col].astype(str)))\n",
    "\n",
    "    # Chi-square test\n",
    "    ct = pd.crosstab(df[encoded_col], df['Churn'])\n",
    "    if ct.shape[0] < 2 or ct.shape[1] < 2 or (ct.sum(axis=1) == 0).any():\n",
    "        print(f\"Skipping chi2 for {col}: not enough variation\")\n",
    "        continue\n",
    "\n",
    "    chi2, p_chi2, dof, expected = chi2_contingency(ct)\n",
    "    print(f\"Chi2 = {chi2:.2f}, p = {p_chi2:.4f} -> {'Significant' if p_chi2 < alpha else 'Not Significant'}\")\n",
    "\n",
    "    # Barplot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(x=col, y='Churn', data=df, ci=None)\n",
    "    plt.title(f'Churn Rate by {col}')\n",
    "    plt.ylabel('Churn Rate')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Pairwise Z-tests\n",
    "    print(\"Pairwise Z-tests:\")\n",
    "    for g1, g2 in combinations(df[encoded_col].unique(), 2):\n",
    "        d1 = df[df[encoded_col] == g1]['Churn']\n",
    "        d2 = df[df[encoded_col] == g2]['Churn']\n",
    "        if min(d1.count(), d2.count()) == 0:\n",
    "            continue\n",
    "        count = [d1.sum(), d2.sum()]\n",
    "        nobs = [d1.count(), d2.count()]\n",
    "        stat, pval = proportions_ztest(count, nobs)\n",
    "        result = \"Significant\" if pval < alpha else \"Not Significant\"\n",
    "        print(f\"  {mapping[g1]} vs {mapping[g2]}: p = {pval:.4f} => {result}\")\n",
    "\n",
    "        results.append({\n",
    "            'Feature': col,\n",
    "            'Group 1': mapping[g1],\n",
    "            'Group 2': mapping[g2],\n",
    "            'p-value': pval,\n",
    "            'Result': result\n",
    "        })\n",
    "\n",
    "# Save pairwise test results\n",
    "pd.DataFrame(results).to_csv(\"pairwise_churn_significance_tests.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b05ac0-d6c8-4773-a466-3387baf01951",
   "metadata": {},
   "source": [
    "\n",
    "## Key Findings from EDA and Statistical Testing\n",
    "\n",
    "- The overall churn rate in the dataset is **~26%**.\n",
    "- **Contract Type** has a significant relationship with churn (p < 0.001).\n",
    "  - Customers on **Month-to-Month** contracts churn at a much higher rate than those on **One-Year** or **Two-Year** contracts.\n",
    "- **Paperless Billing** is associated with higher churn.\n",
    "- **Online Security** and **Tech Support** are strong differentiators:\n",
    "  - Customers **without Online Security** or **without Tech Support** churn at much higher rates.\n",
    "- **Senior Citizens** are slightly more likely to churn compared to non-seniors, though the effect is moderate.\n",
    "- **Payment Method** shows significant differences:\n",
    "  - Customers using **Electronic Check** have a much higher churn rate than those using **Bank Transfers** or **Credit Cards**.\n",
    "\n",
    "### Summary of Significant Features:\n",
    "\n",
    "| Feature            | Significant Relationship with Churn? |\n",
    "|--------------------|-------------------------------------|\n",
    "| Contract           | Yes                                 |\n",
    "| Paperless Billing  | Yes                                 |\n",
    "| Online Security    | Yes                                 |\n",
    "| Tech Support       | Yes                                 |\n",
    "| Payment Method     | Yes                                 |\n",
    "| Senior Citizen     | Moderate                            |\n",
    "| Multiple Lines     | Slight                              |\n",
    "| Internet Service   | Yes                                 |\n",
    "\n",
    "---\n",
    "\n",
    "### **Recommendations Based on EDA**\n",
    "- Encourage customers to switch to **longer-term contracts** (One-Year, Two-Year).\n",
    "- Promote **bundled services** like Online Security and Tech Support to reduce churn.\n",
    "- Target customers with **Electronic Check** payment method for intervention.\n",
    "\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1df333-f600-4b22-862b-632594c859b6",
   "metadata": {},
   "source": [
    "## Step 8: Feature Selection & Preprocessing\n",
    "\n",
    "Based on our EDA and statistical tests, we selected features most relevant to churn. In this step:\n",
    "\n",
    "- We removed irrelevant or leakage columns (e.g., `customerID`, temp encoded features).\n",
    "- Mapped binary categorical values (`Yes`/`No`) to 1/0 for modeling.\n",
    "- Applied one-hot encoding to multi-class categorical features (e.g., `Contract`, `PaymentMethod`) to convert them into numeric format.\n",
    "- Ensured numeric columns (`TotalCharges`, `MonthlyCharges`, `tenure`) were properly converted and imputed where necessary.\n",
    "- Replaced infinite values and filled any remaining missing values with column medians.\n",
    "\n",
    "At this point, all features are numeric and ready for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba01ce0-0091-4449-9607-12b1007a5c88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 7: Feature Selection and Preprocessing\n",
    "\n",
    "# 1. Copy dataset and drop unnecessary columns\n",
    "df_model = df.copy()\n",
    "df_model.drop(columns=[col for col in ['customerID'] if col in df_model.columns], inplace=True)\n",
    "df_model.drop(columns=[col for col in df_model.columns if col.endswith('_encoded')], inplace=True)\n",
    "\n",
    "# 2. Define target and selected features based on EDA\n",
    "target = 'Churn'\n",
    "selected_features = [\n",
    "    'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService',\n",
    "    'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
    "    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n",
    "    'Contract', 'PaperlessBilling', 'PaymentMethod', \n",
    "    'MonthlyCharges', 'TotalCharges'\n",
    "]\n",
    "\n",
    "X = df_model[selected_features].copy()\n",
    "y = df_model[target]\n",
    "\n",
    "# 3. Binary encoding for Yes/No columns\n",
    "binary_cols = [\n",
    "    'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling',\n",
    "    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "    'TechSupport', 'StreamingTV', 'StreamingMovies'\n",
    "]\n",
    "for col in binary_cols:\n",
    "    if col in X.columns:\n",
    "        X[col] = X[col].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "\n",
    "# 4. One-hot encode multi-category columns\n",
    "multi_cat_cols = ['MultipleLines', 'InternetService', 'Contract', 'PaymentMethod']\n",
    "X = pd.get_dummies(X, columns=[col for col in multi_cat_cols if col in X.columns], drop_first=True)\n",
    "\n",
    "# 5. Ensure all numeric columns are valid and clean\n",
    "for col in ['TotalCharges', 'MonthlyCharges', 'tenure']:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    X[col].fillna(X[col].median(), inplace=True)\n",
    "\n",
    "# 6. Final pass for infs or NaNs\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X.fillna(X.median(), inplace=True)\n",
    "\n",
    "# 7. Sanity check\n",
    "print(\"✅ Remaining NaNs:\", X.isnull().sum().sum())  # Should be 0\n",
    "print(\"📊 Data Types:\\n\", X.dtypes.value_counts())  # Should show int64 and float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea3c3e4-14c9-43dc-907f-086f7827c42b",
   "metadata": {},
   "source": [
    "## 🔢 Step 9: Modeling and Evaluation\n",
    "\n",
    "We trained and evaluated three different models to predict customer churn:\n",
    "\n",
    "1. **Logistic Regression**\n",
    "2. **Random Forest**\n",
    "3. **LightGBM**\n",
    "\n",
    "Each model was evaluated using standard classification metrics and ROC/Precision-Recall analysis. For Random Forest and LightGBM, we also tuned the classification threshold to maximize the F1-score.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Logistic Regression\n",
    "\n",
    "- **Model trained** with scaled features (standardization).\n",
    "- Evaluated on default threshold = 0.5.\n",
    "- Key Metrics:\n",
    "  - ROC AUC: High\n",
    "  - Precision: High (identifies churners with confidence)\n",
    "  - Recall: Moderate (misses some churners)\n",
    "\n",
    "---\n",
    "\n",
    "### 🌲 Random Forest\n",
    "\n",
    "- **Trained without scaling** (tree-based models don't require it).\n",
    "- Threshold tuned to **0.24** for optimal F1-score.\n",
    "- Key Metrics:\n",
    "  - Improved Recall compared to Logistic Regression.\n",
    "  - Slight drop in Precision.\n",
    "- Included Precision-Recall vs Threshold plot and Confusion Matrix at selected threshold.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 LightGBM\n",
    "\n",
    "- **Gradient Boosting Model**, efficient and fast.\n",
    "- Threshold tuned to **0.23** to balance precision and recall.\n",
    "- Delivered the **best F1-score** among all models.\n",
    "- Feature importance chart highlights key drivers of churn.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Evaluation Criteria\n",
    "\n",
    "For all models, we used:\n",
    "\n",
    "- **Classification Report** (Precision, Recall, F1)\n",
    "- **Confusion Matrix**\n",
    "- **ROC Curve**\n",
    "- **Precision-Recall Curve**\n",
    "- **Custom threshold tuning** to maximize F1\n",
    "- **Feature Importance** (coefficients or impurity gain)\n",
    "\n",
    "Each model was compared in a summary table to aid final selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a0246b-a99b-4c59-997b-1ccc562b213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train a logistic regression model on the scaled training data\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression Model\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred_log = log_model.predict(X_test_scaled)\n",
    "y_proba_log = log_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluate performance using classification report, confusion matrix, and ROC AUC\n",
    "print(\"📋 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_log)\n",
    "print(\"🧮 Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "#Visualize confusion matrix\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC AUC Score\n",
    "roc_auc = roc_auc_score(y_test, y_proba_log)\n",
    "print(f\"📈 ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# ROC Curve Plot\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_log)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random guess\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b89f1-3761-4dd0-9676-6bfbacc8de04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521fe323-6d19-4a62-a478-13e24a731dd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Feature Importance: Logistic Regression ---\n",
    "\n",
    "# Extract feature names and coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': log_model.coef_[0]\n",
    "})\n",
    "\n",
    "# Add absolute coefficient for sorting\n",
    "feature_importance['Abs_Coefficient'] = feature_importance['Coefficient'].abs()\n",
    "feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    y='Feature', x='Coefficient', \n",
    "    data=feature_importance,\n",
    "    palette='coolwarm'\n",
    ")\n",
    "plt.title('Feature Importance (Logistic Regression)')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16fe38-83ac-4053-bd5a-8c0b65ace88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cea17f-c2c6-4fbb-a18c-34a5899ec252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a54b70-10ca-4e5b-b451-fff3f570e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 9.2: Random Forest Model ---\n",
    "\n",
    "# Train a Random Forest model (doesn't require feature scaling)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate its performance using classification metrics and ROC AUC\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "roc_auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "print(f\"ROC AUC Score: {roc_auc_rf:.4f}\")\n",
    "\n",
    "# Plot ROC curve for visual comparison\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.2f})', color='darkorange')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - Random Forest')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc1d58-1437-4a11-9bf7-ab2f12f5cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision and recall across probability thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba_rf)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, precision[:-1], label='Precision', linestyle='--')\n",
    "plt.plot(thresholds, recall[:-1], label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision-Recall vs Threshold (Random Forest)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a642505-e556-4c26-8428-5501da121e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize best values\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "# Find the threshold that gives the best F1 score (balance between precision and recall)\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_proba_rf >= thresh).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred_thresh)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = thresh\n",
    "\n",
    "print(f\"Best Threshold: {best_threshold:.2f} with F1 Score: {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2212dc2-9f91-416c-8082-3bae9f679c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply optimal threshold (from F1 tuning) to predicted probabilities\n",
    "optimal_threshold = 0.24\n",
    "y_pred_opt = (y_proba_rf >= optimal_threshold).astype(int)\n",
    "\n",
    "# Print classification report to evaluate performance metrics at new threshold\n",
    "print(f\"Random Forest Classification Report (Threshold {optimal_threshold:.2f}):\")\n",
    "print(classification_report(y_test, y_pred_opt))\n",
    "\n",
    "# Visualize the confusion matrix for better interpretation\n",
    "cm_opt = confusion_matrix(y_test, y_pred_opt)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_opt, display_labels=[\"No Churn\", \"Churn\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f'Confusion Matrix at Threshold {optimal_threshold:.2f}')\n",
    "plt.show()\n",
    "\n",
    "# AUC is threshold-independent and remains constant\n",
    "print(f\"Random Forest ROC AUC Score: {roc_auc_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d223212-d7ec-40d0-ac72-775ab80561cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab12df9-53ef-40cf-bc9d-a6f8ff41837d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb1477-442f-45c2-a245-e867a761754e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512136d-a64d-420b-bcaa-6da3acae1fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Initialize LightGBM Model\n",
    "lgb_model = lgb.LGBMClassifier(random_state=42, n_estimators=100)\n",
    "\n",
    "# 2. Fit the Model\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predict Probabilities\n",
    "y_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 4. ROC AUC Score\n",
    "roc_auc_lgb = roc_auc_score(y_test, y_proba_lgb)\n",
    "print(f\"LightGBM ROC AUC Score: {roc_auc_lgb:.4f}\")\n",
    "\n",
    "# 5. Precision-Recall Curve\n",
    "precision_lgb, recall_lgb, thresholds_lgb = precision_recall_curve(y_test, y_proba_lgb)\n",
    "\n",
    "# 6. Find Best Threshold (Maximize F1 Score)\n",
    "best_thresh_lgb = 0\n",
    "best_f1_lgb = 0\n",
    "\n",
    "for thresh in thresholds_lgb:\n",
    "    preds_thresh = (y_proba_lgb >= thresh).astype(int)\n",
    "    f1 = f1_score(y_test, preds_thresh)\n",
    "    if f1 > best_f1_lgb:\n",
    "        best_f1_lgb = f1\n",
    "        best_thresh_lgb = thresh\n",
    "\n",
    "print(f\"Best Threshold for LightGBM: {best_thresh_lgb:.2f} with F1 Score: {best_f1_lgb:.4f}\")\n",
    "\n",
    "# 7. Classification Report at Best Threshold\n",
    "y_pred_lgb_best = (y_proba_lgb >= best_thresh_lgb).astype(int)\n",
    "print(\"LightGBM Classification Report (Best Threshold):\")\n",
    "print(classification_report(y_test, y_pred_lgb_best))\n",
    "\n",
    "# 8. Confusion Matrix at Best Threshold\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cm_lgb = confusion_matrix(y_test, y_pred_lgb_best)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_lgb, display_labels=[\"No Churn\", \"Churn\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(f'Confusion Matrix at Threshold {best_thresh_lgb:.2f}')\n",
    "plt.show()\n",
    "\n",
    "# 9. Precision-Recall vs Threshold Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(thresholds_lgb, precision_lgb[:-1], label='Precision')\n",
    "plt.plot(thresholds_lgb, recall_lgb[:-1], label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision-Recall vs Threshold (LightGBM)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 10. Feature Importance Plot\n",
    "importances = lgb_model.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "feat_imp = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "feat_imp[:20].plot(kind='barh')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Feature Importance (LightGBM)')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028151c-eaa5-494a-ba0c-6f5a8e01125e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1efd00b-7aa6-4122-a0a1-07aa9453e246",
   "metadata": {},
   "source": [
    "## 🔚 Final Model Comparison Summary\n",
    "\n",
    "We evaluated three models — **Logistic Regression**, **Random Forest**, and **LightGBM** — for customer churn prediction, comparing performance using ROC AUC, Accuracy, Precision, Recall, and F1-Score. Thresholds for Random Forest and LightGBM were chosen to maximize the F1-Score.\n",
    "\n",
    "### 📊 Performance Metrics\n",
    "\n",
    "| Metric                     | Logistic Regression (0.50) | Random Forest (0.24) | LightGBM (0.23) |\n",
    "|---------------------------|----------------------------|----------------------|-----------------|\n",
    "| ROC AUC                   | 0.8431                     | 0.8145               | 0.8292          |\n",
    "| Accuracy                  | 0.80                       | 0.73                 | 0.74            |\n",
    "| Precision (Churners)      | 0.64                       | 0.49                 | 0.50            |\n",
    "| Recall (Churners)         | 0.53                       | 0.76                 | 0.81            |\n",
    "| F1-Score (Churners)       | 0.58                       | 0.60                 | 0.62            |\n",
    "\n",
    "### 📝 Summary\n",
    "\n",
    "- **Logistic Regression** achieved the **highest ROC AUC (0.8431)** and **precision (64%)**, but with **lower recall (53%)**, meaning it missed many true churners.\n",
    "- **Random Forest**, tuned at a **0.24 threshold**, improved recall to **76%**, though with reduced precision (**49%**).\n",
    "- **LightGBM**, tuned at a **0.23 threshold**, offered the **best balance**, with **81% recall** and **50% precision**, leading to the **highest F1-Score (62%)**.\n",
    "\n",
    "### ✅ Final Model Selection\n",
    "\n",
    "We recommend **LightGBM** as the final model due to its **superior ability to identify churners**, balancing the business need for **high recall** while maintaining **acceptable precision**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f072efaa-91f1-4980-8609-06b363df0ca9",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d260a009-d5b1-4b91-9457-7159842d073b",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e95db7d-49d8-4973-b8ba-249b85c049b9",
   "metadata": {},
   "source": [
    "## Step 10: Customer Scoring & Risk Segmentation\n",
    "\n",
    "Using the final LightGBM model, we scored all customers with a predicted churn probability and segmented them into actionable risk groups.\n",
    "\n",
    "### 🔢 Scoring\n",
    "- Each customer received a **churn probability** based on the LightGBM model.\n",
    "\n",
    "### 🟡 Risk Segmentation\n",
    "- **High Risk**: Churn probability > 0.6  \n",
    "- **Medium Risk**: 0.3 < Probability ≤ 0.6  \n",
    "- **Low Risk**: Probability ≤ 0.3  \n",
    "\n",
    "This segmentation enables focused retention strategies and helps prioritize outreach efforts.\n",
    "\n",
    "### 📊 Segment Distribution\n",
    "We analyzed the proportion of customers in each segment to understand the size of the potential churn base.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6aee1d-a3aa-4e93-bad9-e104eea7aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score All Customers Using Final Model\n",
    "\n",
    "# Predict churn probabilities using LightGBM\n",
    "customer_churn_scores = lgb_model.predict_proba(X)[:, 1]  # Probability of churn (class 1)\n",
    "\n",
    "# Create a new DataFrame with scores\n",
    "scored_customers = X.copy()\n",
    "scored_customers['Churn_Probability'] = customer_churn_scores\n",
    "\n",
    "# Preview top rows\n",
    "scored_customers[['Churn_Probability']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b60f71-2982-4eea-ac70-bd2435aba1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Risk Segmentation\n",
    "\n",
    "# Define risk bands based on churn probability\n",
    "def risk_segment(prob):\n",
    "    if prob > 0.6:\n",
    "        return 'High Risk'\n",
    "    elif prob > 0.3:\n",
    "        return 'Medium Risk'\n",
    "    else:\n",
    "        return 'Low Risk'\n",
    "\n",
    "# Apply segmentation\n",
    "scored_customers['Risk_Segment'] = scored_customers['Churn_Probability'].apply(risk_segment)\n",
    "\n",
    "# Display segment distribution\n",
    "risk_counts = scored_customers['Risk_Segment'].value_counts()\n",
    "print(\"📊 Risk Segment Distribution:\")\n",
    "print(risk_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20396eb0-df4c-4d1f-9f10-c23fd6751d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature Importance (LightGBM)\n",
    "\n",
    "# Extract feature importances\n",
    "importances = lgb_model.feature_importances_\n",
    "features = X.columns\n",
    "feat_imp = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "feat_imp[:20].plot(kind='barh')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Top 20 Feature Importances (LightGBM)')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate short summary of top 5 features\n",
    "top_features = feat_imp[:5].index.tolist()\n",
    "\n",
    "feature_summary_text = f\"\"\"\n",
    "Top indicators of customer churn based on the LightGBM model are:\n",
    "1. {top_features[0]}\n",
    "2. {top_features[1]}\n",
    "3. {top_features[2]}\n",
    "4. {top_features[3]}\n",
    "5. {top_features[4]}\n",
    "\n",
    "These features are the most influential in predicting customer churn and should be prioritized for business interventions.\n",
    "\"\"\"\n",
    "print(feature_summary_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a66daf-030d-468a-8e34-f3cdac6efb68",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd08923-9aa8-43be-ad4f-ab8f150acf2f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 11: Model Explainability with SHAP\n",
    "\n",
    "To increase trust and interpretability of the model, SHAP (SHapley Additive exPlanations) was used to explain both **global** and **individual** churn predictions.\n",
    "\n",
    "### 🌍 Global Feature Importance\n",
    "- A **summary plot** shows which features most strongly affect predictions across the dataset.\n",
    "- Confirms that features like `tenure`, `Contract_Two year`, and `OnlineSecurity` have the greatest global impact.\n",
    "\n",
    "### 👤 Individual Force Plots\n",
    "- For the top high-risk customers, **SHAP force plots** highlight how specific features pushed their churn risk up or down.\n",
    "- These can support **personalized retention** campaigns by revealing the unique churn factors for each customer.\n",
    "\n",
    "SHAP enhances model transparency, aligns predictions with business logic, and empowers human-in-the-loop decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67d918-e837-445a-a20b-436432e1e120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bb76c6-cef9-4037-b2fe-4a6d106f5859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explanation (Global Feature Impact)\n",
    "\n",
    "# 1. Initialize SHAP explainer using the trained LightGBM model\n",
    "explainer = shap.TreeExplainer(lgb_model)\n",
    "\n",
    "# 2. Compute SHAP values for the test set (binary classification: returns list of arrays)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# 3. Plot global feature importance (mean absolute SHAP values)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e7bf3-cdd6-4f94-864b-a885330366ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9210cc-657f-44f9-904e-04a7dc630bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e579b6-06cc-4c40-866e-f3f333411d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explanation (Local Interpretation for Top High-Risk Test Customers)\n",
    "\n",
    "# Identify top N high-risk customers in the test set\n",
    "top_n = 5\n",
    "high_risk_test = X_test.copy()\n",
    "high_risk_test['Churn_Probability'] = y_proba_lgb\n",
    "high_risk_test_sorted = high_risk_test.sort_values(by='Churn_Probability', ascending=False).head(top_n)\n",
    "\n",
    "# Loop through top N customers and show force plots\n",
    "shap.initjs()\n",
    "\n",
    "for i, idx in enumerate(high_risk_test_sorted.index):\n",
    "    print(f\"\\nCustomer {i+1} - Index {idx}\")\n",
    "    display(\n",
    "        shap.force_plot(\n",
    "            explainer.expected_value[1],          # Expected value for churn class\n",
    "            shap_values[1][X_test.index.get_loc(idx)],  # Correct index inside X_test\n",
    "            X_test.loc[idx]                       # Feature values for that customer\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2192edd-bef1-4746-b647-f3bef8b86798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize JS for SHAP interactive plots\n",
    "shap.initjs()\n",
    "\n",
    "# Force plot for a specific customer (interactive in notebook)\n",
    "customer_idx = 0  # Change index as needed\n",
    "shap.force_plot(\n",
    "    explainer.expected_value[1],\n",
    "    shap_values[1][customer_idx],\n",
    "    X_test.iloc[customer_idx]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ed320f-31d3-43ae-811d-c9e9fc98c273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea501d4f-db40-4271-99a4-b1ca595e6e25",
   "metadata": {},
   "source": [
    "## 🧩 Steps 10 & 11 Summary – Scoring, Segmentation & Model Explainability\n",
    "\n",
    "### 🎯 Customer Scoring & Segmentation (Step 10)\n",
    "\n",
    "Using the final LightGBM model, each customer was scored with a predicted probability of churn. Based on this score, customers were segmented into:\n",
    "\n",
    "- **High Risk**: Probability > 60%\n",
    "- **Medium Risk**: 30–60%\n",
    "- **Low Risk**: ≤ 30%\n",
    "\n",
    "This segmentation enables targeted retention strategies by identifying which customers require immediate attention and which are more stable.\n",
    "\n",
    "### 🔍 Explainability with SHAP (Step 11)\n",
    "\n",
    "To interpret and validate model behavior, SHAP was used at both **global** and **individual** levels:\n",
    "\n",
    "- **Global SHAP Summary**:\n",
    "  - Key churn drivers included: `Contract_2`, `tenure`, `InternetService_1`, `MonthlyCharges`, and `Contract_1`.\n",
    "  - Features associated with short customer lifespan, high cost, and no commitment had the strongest impact on churn predictions.\n",
    "\n",
    "- **Individual SHAP Force Plots** (Top 5 Risk Cases):\n",
    "  - Nearly all top churners were:\n",
    "    - On **month-to-month contracts**\n",
    "    - Had **low tenure**\n",
    "    - Were **senior citizens** with **high monthly charges**\n",
    "    - Subscribed to **fiber optic internet** without clear added value\n",
    "  - These drivers appeared repeatedly, making them strong targets for churn intervention efforts.\n",
    "\n",
    "SHAP added transparency to the model’s decision-making and allowed for **human-aligned, customer-specific action plans**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee19754-9b6a-4e03-ade8-446f96f4f3cb",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c74630-cfc7-4d0c-94cc-7369abbd67e1",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4851c-6679-49d3-a151-53bf9d1a47be",
   "metadata": {},
   "source": [
    "# 🧾 Final Project Summary – Telecom Churn Prediction\n",
    "\n",
    "This end-to-end analysis focused on understanding and predicting customer churn for a telecom business using a mix of exploratory analysis, statistical testing, and machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Key Phases\n",
    "\n",
    "1. **Exploratory Data Analysis & Statistical Testing**\n",
    "   - Identified churn rate (~26%)\n",
    "   - Found that churn was significantly related to contract type, support services, and billing method\n",
    "   - Statistical z-tests and chi-square confirmed visual insights\n",
    "\n",
    "2. **Modeling & Evaluation**\n",
    "   - Compared Logistic Regression, Random Forest, and LightGBM\n",
    "   - LightGBM tuned to threshold 0.23 achieved:\n",
    "     - **ROC AUC**: 0.8292\n",
    "     - **Recall (churners)**: 81%\n",
    "     - **Precision (churners)**: 50%\n",
    "     - **F1 Score**: **62%** (best among all models)\n",
    "\n",
    "3. **Customer Scoring & Segmentation**\n",
    "   - Customers scored on churn probability\n",
    "   - Segmented into high, medium, and low risk\n",
    "\n",
    "4. **Model Explainability (SHAP)**\n",
    "   - Global SHAP plots validated important churn drivers\n",
    "   - Force plots for top 5 high-risk customers revealed shared risk patterns:\n",
    "     - Short tenure\n",
    "     - Month-to-month contract\n",
    "     - High monthly charges\n",
    "     - Fiber optic internet\n",
    "     - Senior citizen status\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Recommendations\n",
    "\n",
    "- Prioritize **contract upgrades** (to one- or two-year terms)\n",
    "- Offer **loyalty incentives** to **high-paying, low-tenure** customers\n",
    "- Bundle or promote **Online Security and Tech Support**\n",
    "- Focus retention campaigns on **High Risk** segment identified by LightGBM scoring\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Final Outcome\n",
    "\n",
    "LightGBM was selected as the best model for production due to its balance of performance and interpretability. The project delivered a fully interpretable churn scoring system, actionable segmentation, and visual insights to inform business strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d1b2f-265c-4fc4-abe3-1367ba06d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'SeniorCitizen': 'Is Senior Citizen',\n",
    "    'Partner': 'Has Partner',\n",
    "    'Dependents': 'Has Dependents',\n",
    "    'tenure': 'Tenure (Months)',\n",
    "    'PhoneService': 'Has Phone Service',\n",
    "    'OnlineSecurity': 'Has Online Security',\n",
    "    'OnlineBackup': 'Has Online Backup',\n",
    "    'DeviceProtection': 'Has Device Protection',\n",
    "    'TechSupport': 'Has Tech Support',\n",
    "    'StreamingTV': 'Has Streaming TV',\n",
    "    'StreamingMovies': 'Has Streaming Movies',\n",
    "    'PaperlessBilling': 'Uses Paperless Billing',\n",
    "    'MonthlyCharges': 'Monthly Charges',\n",
    "    'TotalCharges': 'Total Charges',\n",
    "    \n",
    "    'MultipleLines_No phone service': 'No Phone Service',\n",
    "    'MultipleLines_Yes': 'Has Multiple Lines',\n",
    "    \n",
    "    'InternetService_Fiber optic': 'Fiber Internet',\n",
    "    'InternetService_No': 'No Internet Service',\n",
    "    \n",
    "    'Contract_One year': 'One-Year Contract',\n",
    "    'Contract_Two year': 'Two-Year Contract',\n",
    "    \n",
    "    'PaymentMethod_Credit card (automatic)': 'Pays by Credit Card (Auto)',\n",
    "    'PaymentMethod_Electronic check': 'Pays by Electronic Check',\n",
    "    'PaymentMethod_Mailed check': 'Pays by Mailed Check',\n",
    "    \n",
    "    'Churn_Probability': 'Predicted Churn Probability',\n",
    "    'Risk_Segment': 'Risk Segment'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5809cecd-aae1-4bf0-9cf8-5947282b966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_customers_readable = scored_customers.rename(columns=rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d5771-2457-4b67-8b9b-cdfb2da84e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scored_customers_readable.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9df52b-392b-47eb-bdc1-56797ad93690",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_customers_readable.to_csv(\"churn_scored_customers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc869c3-7f57-4d5d-a031-e7f7ef496f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_customers_readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed96df56-e0d6-4031-b91e-238011e6d977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
